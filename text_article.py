# -*- coding: utf-8 -*-
"""Article.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UAgCCzyZjbVKEccSLr8jVGcBajZnSl5q
"""

#LIBRARY
import numpy as np
import pandas as pd
import re 
import os 
import datetime #code for vs code with tensor board

from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding
from tensorflow.keras import Sequential, Input
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.callbacks import TensorBoard, EarlyStopping

#Data Loading
URL = 'https://raw.githubusercontent.com/susanli2016/PyCon-Canada-2019-NLP-Tutorial/master/bbc-text.csv'

df = pd.read_csv(URL)

#Data Inspection 

df.head() #check few row of data
df.duplicated().sum() #99 duplicated data

#print(df['text'][1000])

#Data Cleaning

for index, data in enumerate(df['text']):
    df['text'][index] = re.sub('\[.*?\]', '', data) #remove ()
    df['text'][index] = re.sub('\(.*?\)', '', data) #remove []
    df['text'][index] = re.sub('[^a-zA-Z]',' ',  df['text'][index]).lower() #remove number and special characters and punctuation

print (df['text'][148])

#Feature Selection
text = df['text']
category = df['category']

#Data Preprocessing
#tokenizer convert text into number

num_words = 10000 #unique number of words in all the sentences
oov_token = '<OOV>' #out of vocab

tokenizer = Tokenizer(num_words = num_words, oov_token = oov_token) #initiate

#training tokenizer
tokenizer.fit_on_texts(text)
word_index = tokenizer.word_index
print(dict(list(word_index.items())[0:1000]))

#to transform the text using tokenizer --> mms.transform
text = tokenizer.texts_to_sequences(text) #checked the number of words, np.unique(text)

#padding (adding zero) # make sure length of every sentence is the same
padded_text = pad_sequences(text, maxlen = 200, padding = 'post', truncating = 'pre')

#one hot encoder # convert ylabel only to 01 or 10
#to instantiate
ohe = OneHotEncoder(sparse = False)
category = ohe.fit_transform(category[::, None]) #[::,None] to expand the dimension 1d TO 3d

# train test split
#expand dimension before feeding to train_test_split
padded_text = np.expand_dims(padded_text, axis = -1) #expand dimension

X_train, X_test, y_train, y_test = train_test_split(padded_text,category,test_size = 0.2, random_state=123)

#Model Development

embedding_layer = 64
model = Sequential()
model.add(Embedding(num_words, embedding_layer))
model.add(LSTM(embedding_layer, return_sequences= True))
model.add(Dropout(0.3))
model.add(LSTM(64))
model.add(Dropout(0.3))
model.add(Dense(5,activation ='softmax'))
model.summary()

model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics =['acc'])

# Commented out IPython magic to ensure Python compatibility.
#TensorBoard callback

LOGS_PATH = os.path.join(os.getcwd(),'logs', datetime.datetime.now().strftime('%Y%m%d-%H%M%S')) #write into folder call 'logs'

ts_callback = TensorBoard(log_dir = LOGS_PATH)
es_callback = EarlyStopping(monitor ='val_loss', patience = 5, verbose =0 , restore_best_weights = True)

# %load_ext tensorboard
# %tensorboard --logdir logs #plot from logs folder

hist = model.fit(X_train, y_train, validation_data = (X_test, y_test), batch_size= 64, epochs = 10, callbacks = [ts_callback,es_callback ])

#Model Analysis 
import matplotlib.pyplot as plt

print(hist.history.keys()) #untuk tahu key apa yang boleh buka the doors

plt.figure()
plt.plot(hist.history['acc'])
plt.plot(hist.history[ 'val_acc'])
plt.legend(['training', 'validation'])
plt.show()

from sklearn.metrics import classification_report, confusion_matrix

y_predicted = model.predict(X_test)

y_predicted = np.argmax(y_predicted, axis =1)
y_test = np.argmax(y_test, axis = 1)

print(classification_report(y_test, y_predicted))
print(confusion_matrix(y_test, y_predicted))